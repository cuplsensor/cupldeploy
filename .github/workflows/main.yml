# This is a basic workflow to help you get started with Actions

name: CI

# Controls when the action will run. 
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # Derived from https://github.com/marketplace/actions/build-and-push-docker-images
  build_nginx_image:
    runs-on: ubuntu-latest
    
    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2
        with:
          submodules: false 
      
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v1
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
          
      - name: Login to DockerHub
        uses: docker/login-action@v1
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
          
      - name: Prepare tag name
        id: prep
        run: |
          TAG="cupl/cupldeploy_nginx:${GITHUB_SHA::8}"
          echo ::set-output name=tag::${TAG}

      - name: Build and push
        id: docker_build
        uses: docker/build-push-action@v2
        with:
          context: ./cupldeploy_nginx
          file: ./cupldeploy_nginx/Dockerfile
          push: true
          tags: |
            cupl/cupldeploy_nginx:latest
            ${{ steps.prep.outputs.tag }}
     
  create_cluster:
    #needs: [build_nginx_image]
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    env: 
      CLUSTERNAME: githubcluster
      SERVICENAME: ghproject4
      REGION: us-east-1
    
    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:          
      # Sets up the ec2-cli. This is needed for deploying the backend to Amazon fargate.
      - uses: marocchino/setup-ecs-cli@v1
        with:
          version: "v1.18.0"
      
        
      - name: Configure the aws cli
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region ${{ env.REGION }}
          
      - name: Find out if the cluster is running.
        id: clustercheck
        run: | 
          CLUSTERINFO=$(aws ecs describe-clusters --cluster ${{ env.CLUSTERNAME }})
          NCLUSTERS=$(echo $CLUSTERINFO | python3 -c "import sys, json; print(len(json.load(sys.stdin)['clusters']))")
          echo ::set-output name=nclusters::${NCLUSTERS}
          
      - name: Configure a cluster
        run: ecs-cli configure --cluster ${{ env.CLUSTERNAME }} --default-launch-type FARGATE --config-name ${{ env.SERVICENAME }}-config --region ${{ env.REGION }}
        
      - name: Configure a profile
        run: ecs-cli configure profile --access-key ${{ secrets.AWS_ACCESS_KEY_ID }} --secret-key ${{ secrets.AWS_SECRET_ACCESS_KEY }} --profile-name tutorial-profile
          
      # Run a set of commands with the ec2-cli to start the cluster.
      - name: Start the cluster
        id: startcluster
        if: steps.clustercheck.outputs.nclusters == 0
        run: |
          CLIOUTPUT=$(ecs-cli up --cluster-config ${{ env.SERVICENAME }}-config --force --ecs-profile tutorial-profile)
          VPC_ID=$(echo "$CLIOUTPUT" |grep "VPC" | cut -d':' -f 2)
          SUBNET1=$(echo "$CLIOUTPUT" |grep "Subnet created" | sed -n 1p | cut -d':' -f 2)
          SUBNET2=$(echo "$CLIOUTPUT" |grep "Subnet created" | sed -n 2p | cut -d':' -f 2)
          echo ::set-output name=vpc_id::${VPC_ID}
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
          echo "SUBNET1=$SUBNET1" >> $GITHUB_ENV
          echo "SUBNET2=$SUBNET2" >> $GITHUB_ENV
          
      - name: Get the security group ID
        if: steps.clustercheck.outputs.nclusters == 0
        run: |
          VPC_ID=${{ steps.startcluster.outputs.vpc_id }}
          SGOUTPUT=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values="$VPC_ID" --region ${{ env.REGION }})
          SG_ID=$(echo $SGOUTPUT | python3 -c "import sys, json; print(json.load(sys.stdin)['SecurityGroups'][0]['GroupId'])")
          aws ec2 authorize-security-group-ingress --group-id $SG_ID --protocol tcp --port 80 --cidr 0.0.0.0/0 --region ${{ env.REGION }}
          echo "SG_ID=$SG_ID" >> $GITHUB_ENV
          
      # Get information on the running cluster
      - name: Get VPC and subnets from secrets
        if: steps.clustercheck.outputs.nclusters == 1
        run: |
          VPC_ID=${{ secrets.VPC_ID }}
          SUBNET1=${{ secrets.SUBNET1 }}
          SUBNET2=${{ secrets.SUBNET2 }}
          SG_ID=${{ secrets.SG_ID }}
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
          echo "SUBNET1=$SUBNET1" >> $GITHUB_ENV
          echo "SUBNET2=$SUBNET2" >> $GITHUB_ENV
          echo "SG_ID=$SG_ID" >> $GITHUB_ENV          
          
      - name: Describe services under a given name.
        id: servicecheck
        run: | 
          CLUSTERSERVICES=$(aws ecs describe-services --cluster ${{ env.CLUSTERNAME }} --service ${{ env.SERVICENAME }})
          NSERVICES=$(echo $CLUSTERSERVICES | python3 -c "import sys, json; print(len(json.load(sys.stdin)['services']))")
          echo ::set-output name=nservices::${NSERVICES}
        
      - name: List services and stop the first. This should be enough (I don't expect more than one service to be running).
        if: steps.servicecheck.outputs.nservices == 1
        run: | 
          aws ecs delete-service --cluster ${{ env.CLUSTERNAME }} --service ${{ env.SERVICENAME }} --force
          sleep 120
          
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2
        with:
          submodules: false
          
      # Find out if a target group exists
      - name: Check if a target group exists for this service. Create it if not.
        id: tgcheck
        run: |
          TGOUTPUT=$(aws elbv2 describe-target-groups --names ${{ env.SERVICENAME }}-tg || aws elbv2 create-target-group --target-type ip --name ${{ env.SERVICENAME }}-tg --protocol HTTP --port 80 --vpc-id ${{env.VPC_ID}} --health-check-protocol HTTP --health-check-port 80 --health-check-enabled)
          echo $TGOUTPUT
          TARGET_GROUP_ARN=$(echo $TGOUTPUT | python3 -c "import sys, json; print(json.load(sys.stdin)['TargetGroups'][0]['TargetGroupArn'])")
          echo "TARGET_GROUP_ARN=$TARGET_GROUP_ARN" >> $GITHUB_ENV
          
      # Get listeners for the load balancer.
      - name: Add a rule to a load balancer listener. This expects that (1) an application load balancer has been set up and (2) and HTTPS listener has been added. 
        run: |
          PRIORITY=$(echo $RANDOM)
          RULEOUTPUT=$(aws elbv2 create-rule --listener-arn ${{ secrets.HTTPS_LISTENER_ARN }} --priority $PRIORITY --conditions '{"Field":"path-pattern","PathPatternConfig":{"Values": ["/${{ env.SERVICENAME }}/*"]}}' --actions Type=forward,TargetGroupArn=${{ env.TARGET_GROUP_ARN }})
          echo $RULEOUTPUT
          
      # Use the docker-compose file to define a service. STEP5. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-cli-tutorial-fargate.html
      - name: Start a service
        run: |
          envsubst < ecs-params.yml.template > ecs-params.yml
          ecs-cli compose --project-name ${{ env.SERVICENAME }} --file docker-compose.aws.yml service up --create-log-groups --cluster-config ${{ env.SERVICENAME }}-config --ecs-profile tutorial-profile --target-group-arn $TARGET_GROUP_ARN --container-name nginx --container-port 80
        env:
          SUBNET1: ${{env.SUBNET1}}
          SUBNET2: ${{env.SUBNET2}}
          SG_ID: ${{env.SG_ID}}
          ADMINAPI_CLIENTSECRET: ${{ secrets.ADMINAPI_CLIENTSECRET }}
          TAGTOKEN_CLIENTSECRET: ${{ secrets.TAGTOKEN_CLIENTSECRET }}
          HASHIDS_SALT: ${{ secrets.HASHIDS_SALT }}
          CSRF_SESSION_KEY: ${{ secrets.CSRF_SESSION_KEY }}
          SECRET_KEY: ${{ secrets.SECRET_KEY }}
          SERVER_NAME: b.cuplsensor.com
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_HOST: ${{ secrets.DB_HOST }}
          TARGET_GROUP_ARN: ${{ env.TARGET_GROUP_ARN }}
          WSB_HOST: localhost
          WSB_PORT: 5001 
          DROP_ON_INIT: False
          RATELIMIT_STORAGE_URL: ${{ secrets.RATELIMIT_STORAGE_URL }}
          RATELIMIT_STRATEGY: ${{ secrets.RATELIMIT_STRATEGY }}
          RATELIMIT_ENABLED: ${{ secrets.RATELIMIT_ENABLED }}

  # This workflow contains a single job called "build"
  build:
    needs: [create_cluster]
    
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2
        with:
          submodules: true

      # Runs a set of commands using the runners shell. Set CI to false to stop create-react-app from erroring on warnings.
      - name: Run a multi-line script
        run: |
          cd ./cuplfrontend/reactapp
          npm install
          export REACT_APP_WSB_ORIGIN=https://b3.cupl.uk
          CI=false npm run build
          
      - uses: jakejarvis/s3-sync-action@master
        with:
          args: --acl public-read --follow-symlinks --delete
        env:
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: 'us-east-1'   # optional: defaults to us-east-1
          SOURCE_DIR: './cuplfrontend/reactapp/build'      # optional: defaults to entire repository
